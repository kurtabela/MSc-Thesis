{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c572a7e7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from itertools import repeat\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import datasets\n",
    "from tokenisemt import MTWordTokenizer\n",
    "from sklearn.cluster import KMeans\n",
    "from numbers import Number\n",
    "from pandas import DataFrame\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "import sys, codecs, numpy\n",
    "\n",
    "tok = MTWordTokenizer()\n",
    "\n",
    "\n",
    "class autovivify_list(dict):\n",
    "    \"\"\"A pickleable version of collections.defaultdict\"\"\"\n",
    "\n",
    "    def __missing__(self, key):\n",
    "        \"\"\"Given a missing key, set initial value to an empty list\"\"\"\n",
    "        value = self[key] = []\n",
    "        return value\n",
    "\n",
    "    def __add__(self, x):\n",
    "        \"\"\"Override addition for numeric types when self is empty\"\"\"\n",
    "        if not self and isinstance(x, Number):\n",
    "            return x\n",
    "        raise ValueError\n",
    "\n",
    "    def __sub__(self, x):\n",
    "        \"\"\"Also provide subtraction method\"\"\"\n",
    "        if not self and isinstance(x, Number):\n",
    "            return -1 * x\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "# def build_word_vector_matrix(vector_file, n_words):\n",
    "\n",
    "\n",
    "def find_word_clusters(labels_array, cluster_labels):\n",
    "    \"\"\" Return the set of words in each cluster\"\"\"\n",
    "    cluster_to_words = autovivify_list()\n",
    "    for c, i in enumerate(cluster_labels):\n",
    "        cluster_to_words[i].append(labels_array[c])\n",
    "    return cluster_to_words\n",
    "\n",
    "\n",
    "def get_embeddings(model, tokeniser, sentences):\n",
    "    input = tokeniser.batch_encode_plus(sentences,\n",
    "                                        max_length=model.embeddings.position_embeddings.num_embeddings,\n",
    "                                        padding=\"max_length\",\n",
    "                                        truncation=True,\n",
    "                                        # return_overflowing_tokens=True,\n",
    "                                        return_tensors=\"pt\",\n",
    "                                        )\n",
    "    output = model(**input)[0]\n",
    "\n",
    "    words = []\n",
    "    embeddings = []\n",
    "    vocabulary = dict(zip(tokeniser.get_vocab().values(), tokeniser.get_vocab().keys()))\n",
    "    \n",
    "    for i, token in enumerate(map(lambda token_id: vocabulary[token_id],\n",
    "                                  [token for instance in input[\"input_ids\"].tolist() for token in instance])):\n",
    "        try:\n",
    "            embedding = output[:, i]\n",
    "            if token in (\"[CLS]\", \"[SEP]\", \"[PAD]\"):\n",
    "                continue\n",
    "            elif token.startswith(\"##\"):\n",
    "                words[-1] += token[2:]\n",
    "                embeddings[-1] = torch.cat((embeddings[-1], embedding), dim=0)\n",
    "            # removes symbols only such as '-'\n",
    "            elif (all(not c.isalnum() for c in token)) or (all(c.isdigit() for c in token)):\n",
    "                continue\n",
    "            else:\n",
    "                words.append(token)\n",
    "                embeddings.append(embedding)\n",
    "        except:\n",
    "            print(words)\n",
    "            print(token)\n",
    "\n",
    "    return words, [embedding.mean(dim=0) for embedding in embeddings]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8ce7a1c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at MLRS/mBERTu were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at MLRS/mBERTu and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "model_name = \"MLRS/mBERTu\"\n",
    "model, tokeniser = AutoModel.from_pretrained(model_name), AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# get_ipython().run_line_magic('cd', 'test_dataset')\n",
    "os.chdir(\"test_dataset\")\n",
    "os.system(\"tfds build\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b78461de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from test_dataset\\test_dataset\\1.0.9\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset test_dataset (test_dataset\\test_dataset\\1.0.9)\n",
      "INFO:absl:Constructing tf.data.Dataset test_dataset for split None, from test_dataset\\test_dataset\\1.0.9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "examples, metadata = tfds.load(name=\"test_dataset\", with_info=True,\n",
    "                               as_supervised=True, download=True, data_dir=\"test_dataset\")\n",
    "train_examples, val_examples = examples['train'], examples['validation']\n",
    "\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "# Maximum sentence length\n",
    "MAX_LENGTH = 50\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "\n",
    "def tokenize_pairs(en, mt):\n",
    "    return en, mt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_batches(ds):\n",
    "    return (\n",
    "        ds\n",
    "            .cache()\n",
    "            .shuffle(BUFFER_SIZE)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "\n",
    "train_batches = make_batches(train_examples)\n",
    "val_batches = make_batches(val_examples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf4e79b",
   "metadata": {},
   "source": [
    "# Step 1: Getting a list of multi-sense words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8616e7d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at MLRS/mBERTu were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at MLRS/mBERTu and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:datasets.builder:Reusing dataset korpus_malti (C:\\Users\\User\\.cache\\huggingface\\datasets\\MLRS___korpus_malti\\shuffled\\4.0.0\\2e6b0b41f8f6d6983c1d502f48e4c0757193d22c2be6100f98dc5058f4099954)\n",
      "WARNING:datasets.builder:Reusing dataset korpus_malti (C:\\Users\\User\\.cache\\huggingface\\datasets\\MLRS___korpus_malti\\shuffled\\4.0.0\\2e6b0b41f8f6d6983c1d502f48e4c0757193d22c2be6100f98dc5058f4099954)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"MLRS/mBERTu\"\n",
    "model, tokeniser = AutoModel.from_pretrained(model_name), AutoTokenizer.from_pretrained(model_name)\n",
    "total_words_en = []\n",
    "total_words_mt = []\n",
    "total_embeddings_en = []\n",
    "total_embeddings_mt = []\n",
    "mt_examples = datasets.load_dataset(\"MLRS/korpus_malti\", \"shuffled\", split=\"train\")\n",
    "en_examples = datasets.load_dataset(\"MLRS/korpus_malti\", \"shuffled\", split=\"train\")\n",
    "# en_examples = datasets.load_dataset(\"wikipedia\", \"20220301.en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "020a6f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Interessat ħafna f'dak kollu li hu relatat mal-midja u x-xandir.\\n\"}\n",
      "{'text': \"Billi l-arranġamenti in kwistjoni huma maħsuba biex inaqqsu l-ispejjeż ta' produzzjoni u l-prezzijiet tal-konsumatur;\\n\"}\n",
      "{'text': \"rapport ex post sal-31 ta' Diċembru 2034.\\n\"}\n",
      "{'text': 'Dawn l-ispezzjonijiet huma ntiżi sabiex jiżviluppaw rikonoxximent komuni u l-interpretazzjoni tal-prassi u r-rekwiżiti.\\n'}\n",
      "{'text': \"Informazzjoni fuq il-progress u r-riżultat ta' l-investigazzjonijiet għandhom ikunu provduti lill-Istati kollha li għandhom interess fi, jew ikunu affetwati minn, l-allegata vjolazzjoni.\\n\"}\n",
      "{'text': 'Għandhom jingħataw ġustifikazzjonijiet raġunati minn min japplika għaliex ma jissodisfax/tissodisfax il-parametri kollha tal-eliġibbiltà u għaliex huma mistħoqqa kunsiderazzjonijiet speċjali.\\n'}\n",
      "{'text': \"benefiċċji applikabbli, kundizzjonijiet u regoli/regolamenti; bdil raġonevoli għall-persuni rreġistrati b'diżabbiltà;\\n\"}\n",
      "{'text': 'Hu stmat li fl-Ewropa biss mal-5,000 tifel u tifla jinħatfu fis-sena.\\n'}\n",
      "{'text': \"ALEX MUSCAT: Qed naħlu l-ħin ta' dan il-Kumitat.\\n\"}\n",
      "{'text': 'L-Anness III\\n'}\n",
      "{'text': 'Test propost mill-Kummissjoni\\n'}\n",
      "{'text': \"Kif isemmi r-Rapport Finali tal-Kummissjoni dwar l-Inkjesta Settorjali tal-Assigurazzjoni għall-Impriżi tal-25 ta' Settembru 200712, prattiki li jinvolvu allinjament tal-premium (bejn ko-(ri)assiguraturi permezz ta' akkordji ta' ko‐(ri)assigurazzjoni ad-hoc) jistgħu jaqgħu fl-ambitu tal-Artikolu 101(1) tat‐Trattat, iżda jistgħu jibbenefikaw mill-eżenzjoni li joffri l-Artikolu 101(3) tat‐Trattat.\\n\"}\n",
      "{'text': \"Burraxka ta' April (maqluba mill-Ingliż) KUMMIEDJA ĊKEJKNA F'ATT WIEĦED Personaġġi: Stella (ta' tmintax-il sena)\\n\"}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(en_examples):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sentence)\n\u001b[1;32m---> 10\u001b[0m     words, embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokeniser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(words) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(embeddings)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m embeddings:\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mget_embeddings\u001b[1;34m(model, tokeniser, sentences)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embeddings\u001b[39m(model, tokeniser, sentences):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m tokeniser\u001b[38;5;241m.\u001b[39mbatch_encode_plus(sentences,\n\u001b[0;32m     51\u001b[0m                                         max_length\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mposition_embeddings\u001b[38;5;241m.\u001b[39mnum_embeddings,\n\u001b[0;32m     52\u001b[0m                                         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m                                         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     56\u001b[0m                                         )\n\u001b[1;32m---> 57\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     59\u001b[0m     words \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     60\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:990\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    981\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    983\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    984\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    985\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    988\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    989\u001b[0m )\n\u001b[1;32m--> 990\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    991\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1002\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1003\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:582\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    573\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    574\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    575\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 582\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    592\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:470\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    460\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    467\u001b[0m ):\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 470\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    477\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:401\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    393\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    399\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    400\u001b[0m ):\n\u001b[1;32m--> 401\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    411\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:329\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    326\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[0;32m    328\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_probs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1226\u001b[0m, in \u001b[0;36mSoftmax.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m   1225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\functional.py:1680\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1678\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1680\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1681\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1682\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"../embeddings_mt.tsv\", \"w+\", encoding=\"utf-8\") as embeddings_mt_file, \\\n",
    "        open(\"../words_mt.tsv\", \"w+\", encoding=\"utf-8\") as words_mt_file, \\\n",
    "        open(\"../embeddings_en.tsv\", \"w+\", encoding=\"utf-8\") as embeddings_en_file, \\\n",
    "        open(\"../words_en.tsv\", \"w+\", encoding=\"utf-8\") as words_en_file:\n",
    "    words_en_file.write(\"word\\tindex\\n\")\n",
    "    words_mt_file.write(\"word\\tindex\\n\")\n",
    "    \n",
    "    for i, sentence in enumerate(en_examples):\n",
    "        words, embeddings = get_embeddings(model, tokeniser, [sentence[\"text\"].strip()])\n",
    "        assert len(words) == len(embeddings)\n",
    "\n",
    "        for embedding in embeddings:\n",
    "            embeddings_en_file.write(\"\\t\".join(str(x) for x in embedding.tolist()) + \"\\n\")\n",
    "            total_embeddings_en.append(embedding.tolist())\n",
    "        for j, word in enumerate(words):\n",
    "            words_en_file.write(f\"{word}\\t{i}_{j}\\n\")\n",
    "            total_words_en.append(word)\n",
    "            \n",
    "            \n",
    "            \n",
    "    for i, sentence in enumerate(mt_examples):\n",
    "        words, embeddings = get_embeddings(model, tokeniser, [sentence[\"text\"].strip()])\n",
    "        assert len(words) == len(embeddings)\n",
    "\n",
    "        for embedding in embeddings:\n",
    "            embeddings_mt_file.write(\"\\t\".join(str(x) for x in embedding.tolist()) + \"\\n\")\n",
    "            total_embeddings_mt.append(embedding.tolist())\n",
    "        for j, word in enumerate(words):\n",
    "            words_mt_file.write(f\"{word}\\t{i}_{j}\\n\")\n",
    "            total_words_mt.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5149681e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alreadySorted = []\n",
    "\n",
    "with open(\"../listOfMultiSenseWords_mt.txt\", \"w+\", encoding=\"utf-8\") as multisensewords_mt, open(\"../listOfMultiSenseWords_en.txt\", \"w+\", encoding=\"utf-8\") as multisensewords_en:\n",
    "    for i, word in enumerate(total_words_en):\n",
    "        embeddingsOfCurrentWord = []\n",
    "\n",
    "        if word in alreadySorted:\n",
    "            continue\n",
    "        alreadySorted.append(word)\n",
    "\n",
    "        embeddingsOfCurrentWord.append(total_embeddings_en[i])\n",
    "\n",
    "        # Cet all embeddings of the same word in diff sentences\n",
    "        for j, futureWord in enumerate(total_words_en[i+1:]):\n",
    "            if futureWord == word:\n",
    "                embeddingsOfCurrentWord.append(total_embeddings_en[j])\n",
    "\n",
    "        n_words = len(embeddingsOfCurrentWord)\n",
    "        if(n_words < 2):\n",
    "            continue\n",
    "\n",
    "        eps = 17\n",
    "        min_samples = 2\n",
    "        ret = DBSCAN(eps=eps, min_samples=min_samples).fit_predict(embeddingsOfCurrentWord)\n",
    "\n",
    "        if len(set(ret)) > 1:\n",
    "            print(ret)\n",
    "            multisensewords_en.write(word+\"\\n\")\n",
    "            \n",
    "            \n",
    "            \n",
    "    for i, word in enumerate(total_words_mt):\n",
    "        embeddingsOfCurrentWord = []\n",
    "\n",
    "        if word in alreadySorted:\n",
    "            continue\n",
    "        alreadySorted.append(word)\n",
    "\n",
    "        embeddingsOfCurrentWord.append(total_embeddings_mt[i])\n",
    "\n",
    "        # Cet all embeddings of the same word in diff sentences\n",
    "        for j, futureWord in enumerate(total_words_mt[i+1:]):\n",
    "            if futureWord == word:\n",
    "                embeddingsOfCurrentWord.append(total_embeddings_mt[j])\n",
    "\n",
    "        n_words = len(embeddingsOfCurrentWord)\n",
    "        if(n_words < 2):\n",
    "            continue\n",
    "\n",
    "        eps = 17\n",
    "        min_samples = 2\n",
    "        ret = DBSCAN(eps=eps, min_samples=min_samples).fit_predict(embeddingsOfCurrentWord)\n",
    "\n",
    "        if len(set(ret)) > 1:\n",
    "            print(ret)\n",
    "            multisensewords_mt.write(word+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a9302",
   "metadata": {},
   "source": [
    "# Step 2:  Mining multisense words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df9872c",
   "metadata": {},
   "source": [
    "Loop each word in each sentence and get the embeddings and save them to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c278578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m words, embeddings \u001b[38;5;241m=\u001b[39m get_embeddings(model, tokeniser, [en[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()])\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m embeddings:\n\u001b[1;32m---> 17\u001b[0m     \u001b[43membeddingsen_file\u001b[49m\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m embedding\u001b[38;5;241m.\u001b[39mtolist()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m     total_embeddings\u001b[38;5;241m.\u001b[39mappend(embedding\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(words):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import test_dataset\n",
    "import tensorflow_text as text\n",
    "\n",
    "with open(\"../embeddings_mt.tsv\", \"w+\", encoding=\"utf-8\") as embeddingsmt_file, \\\n",
    "        open(\"../words_mt.tsv\", \"w+\", encoding=\"utf-8\") as wordsmt_file, \\\n",
    "        open(\"../embeddings_en.tsv\", \"w+\", encoding=\"utf-8\") as embeddingsen_file, \\\n",
    "        open(\"../words_en.tsv\", \"w+\", encoding=\"utf-8\") as wordsen_file:\n",
    "    \n",
    "\n",
    "\n",
    "    for i, en in enumerate(en_examples):\n",
    "        total_embeddings = []\n",
    "        total_words = []\n",
    "        words, embeddings = get_embeddings(model, tokeniser, [en[\"text\"].strip()])\n",
    "\n",
    "        for embedding in embeddings:\n",
    "            embeddingsen_file.write(\"\\t\".join(str(x) for x in embedding.tolist()) + \"\\n\")\n",
    "            total_embeddings.append(embedding.tolist())\n",
    "        for j, word in enumerate(words):\n",
    "            wordsen_file.write(f\"{word}\\t{i}_{j}\\n\")\n",
    "            total_words.append(word)\n",
    "\n",
    "    for i, mt in enumerate(mt_examples):\n",
    "        total_embeddings = []\n",
    "        total_words = []\n",
    "        words, embeddings = get_embeddings(model, tokeniser, [mt[\"text\"].strip()])\n",
    "\n",
    "        for embedding in embeddings:\n",
    "            embeddingsmt_file.write(\"\\t\".join(str(x) for x in embedding.tolist()) + \"\\n\")\n",
    "            total_embeddings.append(embedding.tolist())\n",
    "        for j, word in enumerate(words):\n",
    "            wordsmt_file.write(f\"{word}\\t{i}_{j}\\n\")\n",
    "            total_words.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16071329",
   "metadata": {},
   "source": [
    "Traverse each word in each sentence and check for multisense words. If one is found, get the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eab870",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_multisense_en_info = []\n",
    "found_multisense_mt_info = []\n",
    "with open(\"../listOfMultiSenseWords_en.txt\", \"r\", encoding=\"utf-8\") as multisense_en, \\\n",
    "        open(\"../listOfMultiSenseWords_mt.txt\", \"r\", encoding=\"utf-8\") as multisense_mt:\n",
    "    \n",
    "    en_multisense_words = multisense_en.readlines()\n",
    "    mt_multisense_words = multisense_mt.readlines()\n",
    "\n",
    "    for i, en in enumerate(en_examples):\n",
    "        for multisense_word_pos, word in enumerate(en[\"text\"].split()):\n",
    "            if word in en_multisense_words:\n",
    "                words, embeddings = get_embeddings(model, tokeniser, [en.decode('utf-8').strip()])\n",
    "                found_multisense_en_info.append({'word': word, 'embedding': embeddings[multisense_word_pos, 'sentence': en.decode('utf-8')]})\n",
    "    \n",
    "    for i, mt in enumerate(mt_examples):\n",
    "        for multisense_word_pos, word in enumerate(mt[\"text\"].split()):\n",
    "            if word in mt_multisense_words:\n",
    "                words, embeddings = get_embeddings(model, tokeniser, [mt.decode('utf-8').strip()])\n",
    "                found_multisense_mt_info.append({'word': word, 'embedding': embeddings[multisense_word_pos, 'sentence': mt.decode('utf-8')]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c21ca5b",
   "metadata": {},
   "source": [
    "Get each sentence's embedding and store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5aede7bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "en_embeddings = []\n",
    "en_words = []\n",
    "mt_embeddings = []\n",
    "mt_words = []\n",
    "\n",
    "for i, en in enumerate(en_examples):\n",
    "    words, embeddings = get_embeddings(model, tokeniser, [en[\"text\"].strip()])\n",
    "    en_embeddings.append(embeddings)\n",
    "    en_words.append(words)\n",
    "for i, mt in enumerate(mt_examples):\n",
    "    words, embeddings = get_embeddings(model, tokeniser, [mt[\"text\"].strip()])\n",
    "    mt_embeddings.append(embeddings)\n",
    "    mt_words.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754ebf72",
   "metadata": {},
   "source": [
    "Calculate scores of multisense word and each word in each sentence to see which is closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdee3f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings_en = [e.detach().numpy() for e in en_embeddings]\n",
    "results_en = []\n",
    "for en_multi in found_multisense_en_info:\n",
    "    list_of_sim = []\n",
    "    list_of_sim_info = []\n",
    "    for i, sentence_embedding in enumerate(mt_embeddings):\n",
    "        embeddings_mt = [e.detach().numpy() for e in sentence_embedding]\n",
    "        for j, word_embedding in enumerate(embeddings_mt):\n",
    "            res = cosine_similarity(word_embedding.reshape(1, -1) , en_multi['embedding'].reshape(1, -1))\n",
    "            print(res)\n",
    "            list_of_sim.append(res)\n",
    "            list_of_sim_info.append({'multi_sense_word': en_multi['word'], 'multi_sense_sentence': en_multi['sentence'], 'target_word': mt_words[i][j], 'target_sentence': mt_words[i]})\n",
    "    list_of_biggest_sim_indices = sorted(range(len(list_of_sim)), key=lambda i: list_of_sim[i])[-5:]\n",
    "    for index in list_of_biggest_sim_indices:\n",
    "        results_en.append(list_of_sim_info[index])\n",
    "        \n",
    "        \n",
    "results_mt = []\n",
    "for mt_multi in found_multisense_mt_info:\n",
    "    list_of_sim = []\n",
    "    list_of_sim_info = []\n",
    "    for i, sentence_embedding in enumerate(en_embeddings):\n",
    "        embeddings_en = [e.detach().numpy() for e in sentence_embedding]\n",
    "        for j, word_embedding in enumerate(embeddings_en):\n",
    "            res = cosine_similarity(word_embedding.reshape(1, -1) , mt_multi['embedding'].reshape(1, -1))\n",
    "            print(res)\n",
    "            list_of_sim.append(res)\n",
    "            list_of_sim_info.append({'multi_sense_word': mt_multi['word'], 'multi_sense_sentence': mt_multi['sentence'], 'target_word': en_words[i][j], 'target_sentence': en_words[i]})\n",
    "    list_of_biggest_sim_indices = sorted(range(len(list_of_sim)), key=lambda i: list_of_sim[i])[-5:]\n",
    "    for index in list_of_biggest_sim_indices:\n",
    "        results_mt.append(list_of_sim_info[index])\n",
    "\n",
    "print(results_en)\n",
    "print(results_mt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a72059",
   "metadata": {},
   "source": [
    "# Step 3: Create sentence pairs for each multisense word and create data to be backtranslated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa22b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_mt_en = tf.saved_model.load('translator_mt_en')\n",
    "reloaded_en_mt = tf.saved_model.load('translator_en_mt')\n",
    "\n",
    "for result in results_mt:\n",
    "    source_sentence = result[\"target_sentence\"].replace(result[\"target_word\"], \"<MASK>\")\n",
    "    target_sentence = reloaded_mt_en(source_sentence).numpy().decode('utf-8')\n",
    "    print(source_sentence)\n",
    "    print(target_sentence)\n",
    "    \n",
    "for result in results_en:\n",
    "    source_sentence = result[\"target_sentence\"].replace(result[\"target_word\"], \"<MASK>\")\n",
    "    target_sentence = reloaded_en_mt(source_sentence).numpy().decode('utf-8')\n",
    "    print(source_sentence)\n",
    "    print(target_sentence)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
